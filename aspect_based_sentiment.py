# -*- coding: utf-8 -*-
"""aspect_based_sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JToYTDi1tI1mgfPYheihKeHTDqSqnNtM

# import necessary libraries
"""

# import necessary libraries
import os
import re
import pathlib
import shutil
import random
import requests
import tarfile
import shutil
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import nltk
from nltk.tokenize import word_tokenize
import pandas as pd

"""# Download and extract the IMDB dataset"""

# Download and extract the IMDB dataset
url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'
file_name = 'aclImdb_v1.tar.gz'

with requests.get(url, stream=True) as r:
    r.raise_for_status()
    with open(file_name, 'wb') as f:
        for chunk in r.iter_content(chunk_size=8192):
            f.write(chunk)


with tarfile.open(file_name, 'r:gz') as tar:
    tar.extractall()

# Remove the downloaded tar.gz file
os.remove(file_name)

#Remove the unsupervised training data directory
unsup_dir = 'aclImdb/train/unsup'
if os.path.exists(unsup_dir):
    shutil.rmtree(unsup_dir)

# Download NLTK data
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Prepare directories for validation data
base_dir = pathlib.Path("aclImdb")
val_dir = base_dir / "val"
train_dir = base_dir / "train"

for category in ("neg", "pos"):
    os.makedirs(val_dir / category, exist_ok=True)
    files = os.listdir(train_dir / category)
    random.Random(1337).shuffle(files)
    num_val_samples = int(0.2 * len(files))
    val_files = files[-num_val_samples:]
    for fname in val_files:
        shutil.move(train_dir / category / fname, val_dir / category / fname)

"""## Load the data"""

# Load data using text_dataset_from_directory
batch_size = 32
train_ds = keras.utils.text_dataset_from_directory("aclImdb/train", batch_size=batch_size)
val_ds = keras.utils.text_dataset_from_directory("aclImdb/val", batch_size=batch_size)
test_ds = keras.utils.text_dataset_from_directory("aclImdb/test", batch_size=batch_size)

"""# Text Pre processing and Vectorization

## Text Vectorization
"""

# Define TextVectorization layer
max_features = 20000
embedding_dim = 128
sequence_length = 500

vectorize_layer = layers.TextVectorization(max_tokens=max_features,output_mode='int',output_sequence_length=sequence_length)

# Make a text-only dataset (without labels), then call `adapt`
text_ds = train_ds.map(lambda x, y: x)
vectorize_layer.adapt(text_ds)

# Define some aspect keywords
aspect_keywords = {
    "acting": ["actor", "actress", "acting", "performance", "role", "cast"],
    "story": ["story", "plot", "narrative", "script", "screenplay"],
    "visuals": ["visual", "cinematography", "scene", "shot", "special effects"],
    "sound": ["sound", "music", "score", "audio"],
    "direction": ["director", "direction", "filmmaker", "filmmaking"]
}

# Vectorize the text and extract labels
def extract_text_and_labels(dataset, vectorize_layer):
    texts = []
    labels = []
    for text_batch, label_batch in dataset:
        vectorized_texts = vectorize_layer(text_batch).numpy()
        texts.append(vectorized_texts)
        labels.append(label_batch.numpy())
    return np.concatenate(texts), np.concatenate(labels)

train_texts, train_labels = extract_text_and_labels(train_ds, vectorize_layer)
val_texts, val_labels = extract_text_and_labels(val_ds, vectorize_layer)
test_texts, test_labels = extract_text_and_labels(test_ds, vectorize_layer)

# Tokenize and vectorize the reviews to extract aspects
def extract_aspects(reviews, aspect_keywords, sequence_length, vectorize_layer):
    vocab = vectorize_layer.get_vocabulary()
    word_to_index = {word: index for index, word in enumerate(vocab)}

    aspect_vectors = []
    for review in reviews:
        aspect_vector = np.zeros(sequence_length)
        for i, token_index in enumerate(review[:sequence_length]):
            token = vocab[token_index] if token_index < len(vocab) else ""
            for aspect, keywords in aspect_keywords.items():
                if token in keywords:
                    aspect_vector[i] = 1
        aspect_vectors.append(aspect_vector)
    return np.array(aspect_vectors)

train_aspects = extract_aspects(train_texts, aspect_keywords, sequence_length, vectorize_layer)
val_aspects = extract_aspects(val_texts, aspect_keywords, sequence_length, vectorize_layer)
test_aspects = extract_aspects(test_texts, aspect_keywords, sequence_length, vectorize_layer)

"""# Define the ABSA Model

## Experiment 1 : Define the initial  model
"""

# Define the ABSA model
input_text = keras.Input(shape=(sequence_length,), dtype='int32')
input_aspect = keras.Input(shape=(sequence_length,), dtype='float32')

embedding_layer = layers.Embedding(input_dim=max_features + 1, output_dim=embedding_dim)
embedded_text = embedding_layer(input_text)

lstm_out = layers.LSTM(128, return_sequences=True)(embedded_text)

# Reshape aspect input
aspect_dense = layers.Dense(128, activation='relu')(input_aspect)

attention_out = layers.Attention()([lstm_out, aspect_dense])
avg_pool = layers.GlobalAveragePooling1D()(attention_out)

output = layers.Dense(1, activation='sigmoid')(avg_pool)

absa_model = keras.Model(inputs=[input_text, input_aspect], outputs=output)
absa_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
absa_model.summary()

"""## Experiment 2- Add additional LSTM layer"""

# Define the ABSA model
input_text = keras.Input(shape=(sequence_length,), dtype='int32')
input_aspect = keras.Input(shape=(sequence_length,), dtype='float32')

embedding_layer = layers.Embedding(input_dim=max_features + 1, output_dim=embedding_dim)
embedded_text = embedding_layer(input_text)

lstm_out = layers.LSTM(128, return_sequences=True)(embedded_text)

# Second LSTM layer
lstm_out = layers.LSTM(128, return_sequences=True)(lstm_out)

# Reshape aspect input
aspect_dense = layers.Dense(128, activation='relu')(input_aspect)

attention_out = layers.Attention()([lstm_out, aspect_dense])
avg_pool = layers.GlobalAveragePooling1D()(attention_out)

output = layers.Dense(1, activation='sigmoid')(avg_pool)

absa_model_addLstm = keras.Model(inputs=[input_text, input_aspect], outputs=output)
absa_model_addLstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
absa_model_addLstm.summary()

"""## Experiment 3 : Use Multi-Head Attention"""

# Define the ABSA model
input_text = keras.Input(shape=(sequence_length,), dtype='int32')
input_aspect = keras.Input(shape=(sequence_length,), dtype='float32')

embedding_layer = layers.Embedding(input_dim=max_features + 1, output_dim=embedding_dim)
embedded_text = embedding_layer(input_text)

lstm_out = layers.LSTM(128, return_sequences=True)(embedded_text)

# Reshape aspect input
aspect_dense = layers.Dense(128, activation='relu')(input_aspect)

# Multi-head attention layer
attention_out = layers.MultiHeadAttention(num_heads=4, key_dim=128)(lstm_out, lstm_out)

avg_pool = layers.GlobalAveragePooling1D()(attention_out)

output = layers.Dense(1, activation='sigmoid')(avg_pool)

absa_model_multi = keras.Model(inputs=[input_text, input_aspect], outputs=output)
absa_model_multi.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
absa_model_multi.summary()

"""# Train the Model

## Experiment 1: Train the Model
"""

# Train the ABSA model
epochs = 10
history = absa_model.fit(
    [train_texts, train_aspects],
    train_labels,
    validation_data=([val_texts, val_aspects], val_labels),
    epochs=epochs,
    batch_size=batch_size
)

"""## Experiment 2- Train the model"""

# Train the ABSA model
epochs = 10
history = absa_model_addLstm.fit(
    [train_texts, train_aspects],
    train_labels,
    validation_data=([val_texts, val_aspects], val_labels),
    epochs=epochs,
    batch_size=batch_size
)

"""## Experiment 3: Train the Model"""

# Train the ABSA model
epochs = 10
history = absa_model_multi.fit(
    [train_texts, train_aspects],
    train_labels,
    validation_data=([val_texts, val_aspects], val_labels),
    epochs=epochs,
    batch_size=batch_size
)

"""# Evaluate the Model based on the test data

## Experiment 1: Evaluate the Model
"""

# Evaluate the ABSA model
loss, accuracy = absa_model.evaluate([test_texts, test_aspects], test_labels)
print(f"ABSA Test Accuracy: {accuracy:.2f}")

"""## Experiment 2: Evaluate the Model"""

# Evaluate the ABSA model
loss, accuracy = absa_model_addLstm.evaluate([test_texts, test_aspects], test_labels)
print(f"ABSA Test Accuracy: {accuracy:.2f}")

"""## Experiment 3: Evaluate the Model"""

# Evaluate the ABSA model
loss, accuracy = absa_model_multi.evaluate([test_texts, test_aspects], test_labels)
print(f"ABSA Test Accuracy: {accuracy:.2f}")

"""# Group member contribution"""

data = {
    "Group member": ["SN:8436514", "SN:8353293", "SN:8062675", "SN:8734173"],
    "Assigned task": ["Model development and training", "Data pre-processing", "Related Work", "Related Work"],
    "Contribution mark (out of 10)": [9.5, 8.0, 7.0, 8.0]

}

df = pd.DataFrame(data)
print(df)